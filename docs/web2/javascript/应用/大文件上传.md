# 大文件上传

## 大文件分片上传

**核心思路：** 把大文件切成多个小块，分别上传，服务端再合并

### 前端实现

```js
// 1. 文件切片
const createChunks = (file, chunkSize = 2 * 1024 * 1024) => {
  const chunks = []
  let cur = 0
  
  while (cur < file.size) {
    chunks.push(file.slice(cur, cur + chunkSize))
    cur += chunkSize
  }
  
  return chunks
}

// 2. 上传分片
const uploadChunks = async (file) => {
  const chunks = createChunks(file)
  const fileName = file.name
  
  // 并发上传所有分片
  const requests = chunks.map((chunk, index) => {
    const formData = new FormData()
    formData.append('chunk', chunk)
    formData.append('fileName', fileName)
    formData.append('chunkIndex', index)
    formData.append('totalChunks', chunks.length)
    
    return fetch('/upload-chunk', {
      method: 'POST',
      body: formData
    })
  })
  
  await Promise.all(requests)
  
  // 3. 通知服务端合并
  await fetch('/merge', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ fileName, totalChunks: chunks.length })
  })
}
```

## 断点续传

**核心思路：** 上传前先检查哪些分片已上传，只上传未完成的分片

### 前端实现

```js
// 1. 生成文件唯一标识（使用 spark-md5）
import SparkMD5 from 'spark-md5'

const calculateHash = (file) => {
  return new Promise((resolve) => {
    const chunks = createChunks(file)
    const spark = new SparkMD5.ArrayBuffer()
    const reader = new FileReader()
    let currentChunk = 0
    
    reader.onload = (e) => {
      spark.append(e.target.result)
      currentChunk++
      
      if (currentChunk < chunks.length) {
        reader.readAsArrayBuffer(chunks[currentChunk])
      } else {
        resolve(spark.end())
      }
    }
    
    reader.readAsArrayBuffer(chunks[0])
  })
}

// 2. 断点续传逻辑
const uploadWithResume = async (file) => {
  const chunks = createChunks(file)
  const fileHash = await calculateHash(file)
  
  // 检查已上传的分片
  const { uploadedChunks } = await fetch('/verify', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ fileHash, fileName: file.name })
  }).then(res => res.json())
  
  // 只上传未完成的分片
  const requests = chunks
    .map((chunk, index) => ({ chunk, index }))
    .filter(({ index }) => !uploadedChunks.includes(index))
    .map(({ chunk, index }) => {
      const formData = new FormData()
      formData.append('chunk', chunk)
      formData.append('fileHash', fileHash)
      formData.append('chunkIndex', index)
      
      return fetch('/upload-chunk', {
        method: 'POST',
        body: formData
      })
    })
  
  await Promise.all(requests)
  
  // 合并
  await fetch('/merge', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ fileHash, fileName: file.name })
  })
}
```

## 进阶优化

### 1. 上传进度

```js
const uploadWithProgress = async (file) => {
  const chunks = createChunks(file)
  let uploadedSize = 0
  
  const requests = chunks.map((chunk, index) => {
    return new Promise((resolve) => {
      const xhr = new XMLHttpRequest()
      
      xhr.upload.onprogress = (e) => {
        uploadedSize += e.loaded
        const progress = Math.floor((uploadedSize / file.size) * 100)
        console.log(`上传进度: ${progress}%`)
      }
      
      xhr.onload = () => resolve()
      
      const formData = new FormData()
      formData.append('chunk', chunk)
      formData.append('chunkIndex', index)
      
      xhr.open('POST', '/upload-chunk')
      xhr.send(formData)
    })
  })
  
  await Promise.all(requests)
}
```

### 2. 并发控制

```js
// 限制同时上传的分片数量
const uploadWithLimit = async (chunks, limit = 3) => {
  const results = []
  const executing = []
  
  for (const [index, chunk] of chunks.entries()) {
    const p = uploadChunk(chunk, index)
    results.push(p)
    
    if (limit <= chunks.length) {
      const e = p.then(() => executing.splice(executing.indexOf(e), 1))
      executing.push(e)
      
      if (executing.length >= limit) {
        await Promise.race(executing)
      }
    }
  }
  
  return Promise.all(results)
}
```

### 3. 秒传功能

```js
// 上传前先验证文件是否已存在
const instantUpload = async (file) => {
  const fileHash = await calculateHash(file)
  
  const { exists } = await fetch('/check-file', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ fileHash })
  }).then(res => res.json())
  
  if (exists) {
    console.log('文件已存在，秒传成功')
    return
  }
  
  // 不存在则正常上传
  await uploadWithResume(file)
}
```

## OSS vs 自己实现

### 使用 OSS（阿里云/腾讯云）

**优点：**
- 开箱即用，SDK 封装好分片上传、断点续传
- CDN 加速，全球节点
- 高可用、高并发，不用担心服务器压力
- 自动备份、容灾

**缺点：**
- 需要付费（流量费、存储费）
- 依赖第三方服务

```js
// 阿里云 OSS 示例
import OSS from 'ali-oss'

const client = new OSS({
  region: 'oss-cn-hangzhou',
  accessKeyId: 'your-key',
  accessKeySecret: 'your-secret',
  bucket: 'your-bucket'
})

// 自动分片上传
await client.multipartUpload('file.zip', file, {
  progress: (p) => console.log(p),
  checkpoint: checkpointData // 断点续传
})
```

### 自己实现

**优点：**
- 完全可控，可定制化
- 不依赖第三方，数据安全
- 无额外费用

**缺点：**
- 开发成本高
- 需要自己处理并发、存储、容灾
- 服务器带宽和性能限制

**选择建议：**
- 生产环境、大流量：优先 OSS
- 敏感数据、内网环境：自己实现

## 流程总结

1. 通过 `file.slice()` 进行文件切片，使用 MD5 计算文件 hash 作为唯一标识
2. 并发上传所有分片（可控制并发数）
3. 上传完成后通知后端合并分片
4. 断点续传：上传前先请求服务端校验已上传的分片，只传未完成的
5. 秒传：根据文件 hash 判断服务端是否已存在，存在则跳过上传

## 要点

1. **为什么要分片？** 避免大文件上传超时、支持断点续传
2. **如何生成文件唯一标识？** 使用 MD5/SHA256 对文件内容计算 hash
3. **如何实现断点续传？** 上传前验证已上传分片，只传未完成的
4. **并发上传如何控制？** Promise.race + 队列控制并发数
5. **如何防止重复上传？** 文件 hash 作为唯一标识，上传前检查服务端是否已存在
